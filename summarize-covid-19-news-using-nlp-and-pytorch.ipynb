{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c6c2a3",
   "metadata": {},
   "source": [
    "# Summarizing Covid-19 News Using NLP and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e16c6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os, glob\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "webhose_2019_12 = '16119_db21c91a1ab47385bb13773ed8238c31/16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json'\n",
    "webhose_2020_01 = '16119_db21c91a1ab47385bb13773ed8238c31/16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d47fd54",
   "metadata": {},
   "source": [
    "## Download and extract the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13302b",
   "metadata": {},
   "source": [
    "Read each of those files, extract the value of the text key and title key from those objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b7d80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "target = []\n",
    "for filename in [webhose_2019_12, webhose_2020_01]:\n",
    "    with open(filename, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        dataset.append(result['text'])\n",
    "        target.append(result['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c8fa0",
   "metadata": {},
   "source": [
    "The length of the list dataset and target will be 94403. So essentially our dataset size is about 100K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d81c2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94403, 94403)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd3660",
   "metadata": {},
   "source": [
    "## Text cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18f55891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contraction_hashmap import contraction_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6dcb4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.split() # convert have'nt -> have not\n",
    "    for i in range(len(text)):\n",
    "        word = text[i]\n",
    "        if word in contraction_map:\n",
    "            text[i] = contraction_map[word]\n",
    "    text = \" \".join(text)\n",
    "    text = text.split()\n",
    "    newtext = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            newtext.append(word)\n",
    "    text = \" \".join(newtext)\n",
    "    text = text.replace(\"'s\",'') # convert your's -> your\n",
    "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
    "    text = re.sub(r'\\.',' . ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0bcec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [preprocess(text) for text in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c52c26b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94403"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71a5f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [preprocess(text) for text in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93b3ad60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94403"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "392758a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text = 600\n",
    "max_len_target = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "da1cf67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    if(len(target[i].split())<=max_len_target and len(dataset[i].split())<=max_len_text):\n",
    "        short_text.append(dataset[i])\n",
    "        short_summary.append(target[i])\n",
    "\n",
    "temp_df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d0a2a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA launches app for health care professionals...</td>\n",
       "      <td>FDA launches app for health care professionals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Of all of Regina Yan ’s many traits, an open m...</td>\n",
       "      <td>C-Suite Awards: Regina Yan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The CURE ID app allows clinicians to share and...</td>\n",
       "      <td>FDA Launches Infectious Disease Crowdsourcing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The DSB is composed of representatives from tw...</td>\n",
       "      <td>Drug Safety Oversight Board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Centre for Health Protection (CHP) of the ...</td>\n",
       "      <td>Suspected MERS case reported</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  FDA launches app for health care professionals...   \n",
       "1  Of all of Regina Yan ’s many traits, an open m...   \n",
       "2  The CURE ID app allows clinicians to share and...   \n",
       "3  The DSB is composed of representatives from tw...   \n",
       "4  The Centre for Health Protection (CHP) of the ...   \n",
       "\n",
       "                                             summary  \n",
       "0  FDA launches app for health care professionals...  \n",
       "1                         C-Suite Awards: Regina Yan  \n",
       "2  FDA Launches Infectious Disease Crowdsourcing ...  \n",
       "3                        Drug Safety Oversight Board  \n",
       "4                       Suspected MERS case reported  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "650e8920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64893 entries, 0 to 64892\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     64893 non-null  object\n",
      " 1   summary  64893 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1014.1+ KB\n"
     ]
    }
   ],
   "source": [
    "temp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e2338c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = temp_df[temp_df['summary'].str.strip().astype(bool)]\n",
    "df = newdf[newdf['text'].str.strip().astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adac63a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62358 entries, 0 to 64892\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     62358 non-null  object\n",
      " 1   summary  62358 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5a30a",
   "metadata": {},
   "source": [
    "## Text feature generation\n",
    "\n",
    "Now that we have done the text cleanup, we need to convert the text into numerical representations to be used by the model. This process is called feature generation. There are different ways to generate features out of text data. Here we will use one-hot vector[3] technique with some tweaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12f56c7",
   "metadata": {},
   "source": [
    "### Define a class Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "319f3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae39a1",
   "metadata": {},
   "source": [
    "### Create a hashmap word2index\n",
    "\n",
    "This keeps track of when each word first appeared in the text. This is for both the X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6b8365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d22e8c",
   "metadata": {},
   "source": [
    "### Create a hashmap index2word to keep track of which index is which word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d092ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b025919",
   "metadata": {},
   "source": [
    "### Create a separate hashmap word2count\n",
    "\n",
    "This is to count the number of occurrences of each word. We will need this later to replace rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "acefd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d2665",
   "metadata": {},
   "source": [
    "### Define SOS_token = 0 and EOS_token = 1\n",
    "\n",
    "We will need to mark the start of the sentence and end of the sentence for all of the sentences in the target list. So, we need to use some special token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "408d1755",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afad340",
   "metadata": {},
   "source": [
    "## Make the features ready for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed71f13",
   "metadata": {},
   "source": [
    "### Define a function readData(text, summary) \n",
    "\n",
    "This takes text and summary as input. Here text and summary are two lists of strings. When we call this readData function, we will call it with our cleaned data X and Y respectively. This function does the following operations:\n",
    "Creates a tuple from text and summary as in pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "Creates input and output object by passing text and summary to the Lang class Note that we are only creating objects here. Not executing any other functions from the Lang class.\n",
    "Return input, output, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6c564565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(text, summary):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "\n",
    "    \n",
    "    input_lang = Lang(text)\n",
    "    output_lang = Lang(summary)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ef390",
   "metadata": {},
   "source": [
    "### Define a function prepareData that takes list(df['text']) and list(df['summary']) as input.\n",
    "\n",
    "This prepareData function calls readData(X,Y) and gets back input, output, and pairs\n",
    "For each item in the pairs list, we will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "13249807",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for pair in pairs:\n",
    "    input.addSentence(pair[0])\n",
    "    output.addSentence(pair[1])\n",
    "# return input, output, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0164ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2):\n",
    "    input_lang, output_lang, pairs = readData(lang1, lang2)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fd73b307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 94403 sentence pairs\n",
      "Counting words...\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d7f38",
   "metadata": {},
   "source": [
    "## Deliverable\n",
    "\n",
    "The deliverable is a Jupyter Notebook documenting your workflow. The end result of this notebook is a list of pairs of sentences. The 1st column in each row of this list is the text sentence and the 2nd column is the target/summary sentence. A sample output below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3a016c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dublin swine healthcare market  growth trends forecast  5 . 2 . 2 coccidiosis 5 . 2 . 3 respiratory diseases 5 . 2 . 4 swine dysentery 5 . 2 . 5 porcine parvovirus 5 . 2 . 6 others 5 . 3 geography 5 . 3 . 1 north america 5 . 3 . 2 europe 5 . 3 . 3 asiapacific 5 . 3 . 4 middle east  africa 5 . 3 . 5 south america 6 competitive landscape 6 . 1 company profiles 6 . 1 . 1 abaxis 6 . 1 . 2 bayer animal health 6 . 1 . 3 boehringer ingelheim 6 . 1 . 4 ceva animal health inc .  6 . 1 . 5 elanco 6 . 1 . 6 idvet 6 . 1 . 7 merck animal health 6 . 1 . 8 merial 6 . 1 . 9 vetoquinol s . a .  6 . 1 . 10 virbac 6 . 1 . 11 zoetis animal healthcare 7 market opportunities future trends information report visit httpswww . researchandmarkets . comrshhuje research markets also offers custom research services providing focused comprehensive tailored research .  contact researchandmarkets . com laura wood senior press manager pressresearchandmarkets . com e . s . t office hours call 19173000470 u . s . can toll free call 18005268630 gmt office hours call 35314168900\n",
      "\n",
      "global swine healthcare market products diseases  geography  forecast 2024\n"
     ]
    }
   ],
   "source": [
    "print(pairs[0][0])\n",
    "print()\n",
    "print(pairs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4dabfe",
   "metadata": {},
   "source": [
    "## Build an Attention Based Deep Learning Model for Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f554ee8",
   "metadata": {},
   "source": [
    "### Define a Sequence-to-Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "97ded919",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max_len_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93650da5",
   "metadata": {},
   "source": [
    "#### Define the encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e9509e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e38674",
   "metadata": {},
   "source": [
    " #### Define the class AttnDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "775bb658",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130945d",
   "metadata": {},
   "source": [
    "#### Convert the training data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "17544242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03121598",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [tensorsFromPair(pair) for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9cb0c7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d01f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
