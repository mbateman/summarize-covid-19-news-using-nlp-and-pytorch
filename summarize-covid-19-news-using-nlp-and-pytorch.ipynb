{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28670142",
   "metadata": {},
   "source": [
    "# Summarizing Covid-19 News Using NLP and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ea67bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os, glob\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bdd265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7fac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "webhose_2019_12 = 'datasubset/16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json'\n",
    "webhose_2020_01 = 'datasubset/16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919a2de",
   "metadata": {},
   "source": [
    "## Extract News Data from the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a7621",
   "metadata": {},
   "source": [
    "### Download and extract the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac48fd",
   "metadata": {},
   "source": [
    "Read each of those files, extract the value of the text key and title key from those objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a27771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "target = []\n",
    "for filename in [webhose_2019_12, webhose_2020_01]:\n",
    "    with open(filename, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        dataset.append(result['text'])\n",
    "        target.append(result['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739234d6",
   "metadata": {},
   "source": [
    "The length of the list dataset and target will be 94403. So essentially our dataset size is about 100K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f516f421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94403, 94403)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b229d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Global Swine Healthcare Market by Products, Diseases & Geography – Forecast to 2024',\n",
       " 'FDA launches app for health care professionals to report novel uses of existing medicines for patients with difficult-to-treat infectious diseases',\n",
       " 'C-Suite Awards: Regina Yan',\n",
       " 'FDA Launches Infectious Disease Crowdsourcing App for Clinicians FDA Launches Infectious Disease Crowdsourcing App for Clinicians',\n",
       " 'Drug Safety Oversight Board',\n",
       " 'How Prepared Are We For The Next Pandemic? Not Very, Experts Show',\n",
       " 'Suspected MERS case reported',\n",
       " 'Factors associated with and barriers to disclosure of a sexual assault to formal on-campus resources among college students - Mennicke A, Bowling J, Gromer J, Ryan C.',\n",
       " \"The effect of university students' violence tendency on their attitude towards domestic violence and the factors affecting domestic violence attitudes - Yagiz R, Sevil U, Guner Ö.\",\n",
       " 'YoYo Discusses Career, Teaching and Female Empowerment']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e3387",
   "metadata": {},
   "source": [
    "### Text cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5f3f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contraction_hashmap import contraction_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03a3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.split() # convert have'nt -> have not\n",
    "    for i in range(len(text)):\n",
    "        word = text[i]\n",
    "        if word in contraction_map:\n",
    "            text[i] = contraction_map[word]\n",
    "    text = \" \".join(text)\n",
    "    text = text.split()\n",
    "    newtext = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            newtext.append(word)\n",
    "    text = \" \".join(newtext)\n",
    "    text = text.replace(\"'s\",'') # convert your's -> your\n",
    "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
    "    text = re.sub(r'\\.',' . ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ad25071",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [preprocess(text) for text in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afde2f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94403"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5040170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [preprocess(text) for text in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90ddda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94403"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1947d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text = 600\n",
    "max_len_target = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05c99e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    if(len(target[i].split())<=max_len_target and len(dataset[i].split())<=max_len_text):\n",
    "        short_text.append(dataset[i])\n",
    "        short_summary.append(target[i])\n",
    "\n",
    "temp_df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee7bb716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA launches app for health care professionals...</td>\n",
       "      <td>FDA launches app for health care professionals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Of all of Regina Yan ’s many traits, an open m...</td>\n",
       "      <td>C-Suite Awards: Regina Yan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The CURE ID app allows clinicians to share and...</td>\n",
       "      <td>FDA Launches Infectious Disease Crowdsourcing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The DSB is composed of representatives from tw...</td>\n",
       "      <td>Drug Safety Oversight Board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Centre for Health Protection (CHP) of the ...</td>\n",
       "      <td>Suspected MERS case reported</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  FDA launches app for health care professionals...   \n",
       "1  Of all of Regina Yan ’s many traits, an open m...   \n",
       "2  The CURE ID app allows clinicians to share and...   \n",
       "3  The DSB is composed of representatives from tw...   \n",
       "4  The Centre for Health Protection (CHP) of the ...   \n",
       "\n",
       "                                             summary  \n",
       "0  FDA launches app for health care professionals...  \n",
       "1                         C-Suite Awards: Regina Yan  \n",
       "2  FDA Launches Infectious Disease Crowdsourcing ...  \n",
       "3                        Drug Safety Oversight Board  \n",
       "4                       Suspected MERS case reported  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd40fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64893 entries, 0 to 64892\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     64893 non-null  object\n",
      " 1   summary  64893 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1014.1+ KB\n"
     ]
    }
   ],
   "source": [
    "temp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52b8743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = temp_df[temp_df['summary'].str.strip().astype(bool)]\n",
    "df = newdf[newdf['text'].str.strip().astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dafbb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62358 entries, 0 to 64892\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     62358 non-null  object\n",
      " 1   summary  62358 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97717a6f",
   "metadata": {},
   "source": [
    "### Text feature generation\n",
    "\n",
    "Now that we have done the text cleanup, we need to convert the text into numerical representations to be used by the model. This process is called feature generation. There are different ways to generate features out of text data. Here we will use one-hot vector[3] technique with some tweaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774afebe",
   "metadata": {},
   "source": [
    "### Define a class Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "182009c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252cce7f",
   "metadata": {},
   "source": [
    "### Make the features ready for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2044c1a",
   "metadata": {},
   "source": [
    "### Define a function readData(text, summary) \n",
    "\n",
    "This takes text and summary as input. Here text and summary are two lists of strings. When we call this readData function, we will call it with our cleaned data X and Y respectively. This function does the following operations:\n",
    "Creates a tuple from text and summary as in pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "Creates input and output object by passing text and summary to the Lang class Note that we are only creating objects here. Not executing any other functions from the Lang class.\n",
    "Return input, output, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f23b312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(text, summary):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "    \n",
    "    input_lang = Lang(text)\n",
    "    output_lang = Lang(summary)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549943e",
   "metadata": {},
   "source": [
    "### Define a function prepareData that takes list(df['text']) and list(df['summary']) as input.\n",
    "\n",
    "This prepareData function calls readData(X,Y) and gets back input, output, and pairs\n",
    "For each item in the pairs list, we will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c2e0caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2):\n",
    "    input_lang, output_lang, pairs = readData(lang1, lang2)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04995f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 62358 sentence pairs\n",
      "Counting words...\n",
      "[\"WHO chief to convene emergency panel to discuss coronavirus SC starts hearing petitions challenging Art 370 abrogation\\nNew Delhi [India], Jan 22 (ANI): Supreme Court's five-judge Constitution Bench, headed by Justice NV Ramana, on Wednesday started hearing many petitions challenging abrogation of Article 370 in Jammu and Kashmir.\\nThe petitions were filed after the central government scrapped Article 370 in August last year and bifurcated Jammu and Kashmir into two Union Territories -- Jammu and Kashmir and Ladakh. (ANI) SC starts hearing petitions challenging Art 370 abrogation SC starts hearing petitions challenging Art 370 abrogation ANI 22nd January 2020, 16:55 GMT+11\\nNew Delhi [India], Jan 22 (ANI): Supreme Court's five-judge Constitution Bench, headed by Justice NV Ramana, on Wednesday started hearing many petitions challenging abrogation of Article 370 in Jammu and Kashmir.\\nThe petitions were filed after the central government scrapped Article 370 in August last year and bifurcated Jammu and Kashmir into two Union Territories -- Jammu and Kashmir and Ladakh. (ANI) Read This Next\", 'SC starts hearing petitions challenging Art 370 abrogation']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(list(df['text']), list(df['summary']))\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dce977",
   "metadata": {},
   "source": [
    "## Deliverable\n",
    "\n",
    "The deliverable is a Jupyter Notebook documenting your workflow. The end result of this notebook is a list of pairs of sentences. The 1st column in each row of this list is the text sentence and the 2nd column is the target/summary sentence. A sample output below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95a9d6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDA launches app for health care professionals to report novel uses of existing medicines for patients with difficult-to-treat infectious diseases FDA launches app for health care professionals to report novel uses of existing medicines for patients with difficult-to-treat infectious diseases Authors: FDA Pinworms of the red howler monkey (Alouatta seniculus) in Colombia: gathering the pieces of the pinworm-primate puzzle Publication date: Available online 4 December 2019Source: International Journal for Parasitology: Parasites and WildlifeAuthor(s): Brenda Solórzano-García, Andrés Link Ospina, Silvia Rondón, Gerardo Pérez-Ponce de LeónAbstractPinworms of primates are believed to be highly host specific parasites, forming co-evolutionary associations with their hosts. In order to assess the strength and reach of such evolutionary links, we need to have a broad understanding of the pinworm diversity associated with primates. Here, we employed an integrative taxonomic approach to assess pinworm divers... Parasitology Influences of cyclosporin A and non-immunosuppressive derivatives on cellular cyclophilins and viral nucleocapsid protein during human coronavirus 229E replication Publication date: January 2020Source: Antiviral Research, Volume 173Author(s): Yue Ma-Lauer, Yu Zheng, Miroslav Malešević, Brigitte von Brunn, Gunter Fischer, Albrecht von BrunnAbstractThe well-known immunosuppressive drug cyclosporin A inhibits replication of various viruses including coronaviruses by binding to cellular cyclophilins thus inactivating their cis-trans peptidyl-prolyl isomerase function. Viral nucleocapsid proteins are inevitable for genome encapsidation and replication. Here we demonstrate the interaction between the N protein of HCoV-229E and cyclophilin A, not cyclophilin B. Cyclophilin inhibitor...\n",
      "\n",
      "FDA launches app for health care professionals to report novel uses of existing medicines for patients with difficult-to-treat infectious diseases\n"
     ]
    }
   ],
   "source": [
    "print(pairs[0][0])\n",
    "print()\n",
    "print(pairs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b18669",
   "metadata": {},
   "source": [
    "## Build an Attention Based Deep Learning Model for Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c9fca",
   "metadata": {},
   "source": [
    "### Define a Sequence-to-Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d3cbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55cbc6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max_len_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298cd05",
   "metadata": {},
   "source": [
    "#### Define the encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ceceb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0b554",
   "metadata": {},
   "source": [
    " #### Define the class AttnDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "387ccf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ff523",
   "metadata": {},
   "source": [
    "#### Convert the training data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d295965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b6184",
   "metadata": {},
   "source": [
    "### Train a Sequence-to-Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d8de4",
   "metadata": {},
   "source": [
    "#### The train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf3fbbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_length):\n",
    "        try:\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "            encoder_outputs[i] = encoder_output[0, 0]\n",
    "        except IndexError:\n",
    "            print('Index Error in train')\n",
    "            print('index=',i)\n",
    "            print('input_length', input_length)\n",
    "            print('target_length', target_length)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982b26a",
   "metadata": {},
   "source": [
    "#### The trainIters method\n",
    "\n",
    "The trainIters method has 4 important parameters. You can have more parameters for debugging/logging purposes.\n",
    "\n",
    "- encoder: The encoder object of the Encoder class\n",
    "- decoder: The decoder object of the AttnDecoder class\n",
    "- num_iters: Number of iterations you want to train using the train method. This is an integer number.\n",
    "- learning_rate: Learning rate hyperparameter for your neural network. Feel free to use a default value for this parameter.\n",
    "\n",
    "We need to define the optimizers for encoder and decoder objects. These optimizers are gradient descent optimizers.\n",
    "\n",
    "Convert the training pairs to tensors\n",
    "\n",
    "Define the loss function. We are solving a classification problem, so we have to use a loss function that is commonly used in classification problems: log loss. In particular, here we will use negative log loss as criterion = nn.NLLLoss(). Refer to Understanding how LSTM works.\n",
    "\n",
    "Finally, we will call the train method num_iters times.\n",
    "\n",
    "We can save this loss output to look at the training loss over time. This helps us to debug the model and makes sure our model is improving over time.\n",
    "\n",
    "Add some log messages in the beginning and end of this method to keep track of the start and end of training such as starting training ... and end training ... as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d596be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10019ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable2numpy(var):\n",
    "    \"\"\" For tensorboard visualization \"\"\"\n",
    "    return var.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb8af790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_tensorboard(writer, loss, total_loss, encoder, decoder):\n",
    "    writer.add_scalars('loss in each iteration', {'loss':loss})\n",
    "    writer.add_scalars('total loss', {'total loss':total_loss})\n",
    "    \n",
    "    global_step = 1000\n",
    "\n",
    "    for name, param in encoder.named_parameters():\n",
    "        name = name.replace('.', '/')\n",
    "        writer.add_histogram('encoder/{}'.format(name), variable2numpy(param), global_step, bins='auto')\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram('encoder/{}/grad'.format(name), variable2numpy(param.grad), global_step, bins='auto')\n",
    "    \n",
    "    for name, param in decoder.named_parameters():\n",
    "        name = name.replace('.', '/')\n",
    "        writer.add_histogram('decoder/{}'.format(name), variable2numpy(param), global_step, bins='auto')\n",
    "        if param.grad is not None:\n",
    "            writer.add_histogram('decoder/{}/grad'.format(name), variable2numpy(param.grad), global_step, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97f3c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    print('Starting training ...')\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    total_loss = 0\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        total_loss +=loss\n",
    "        write_to_tensorboard(writer, loss, total_loss, encoder, decoder)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('Iteration', iter)\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    print('Stopping training ...')\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87385677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db51c4",
   "metadata": {},
   "source": [
    "#### Now actually train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c86d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n",
      "Iteration 10\n",
      "5m 34s (- 552m 35s) (10 1%) 8.8203\n",
      "Iteration 20\n",
      "13m 23s (- 656m 2s) (20 2%) 7.8490\n",
      "Iteration 30\n",
      "25m 43s (- 831m 57s) (30 3%) 3.6882\n",
      "Iteration 40\n",
      "37m 9s (- 891m 52s) (40 4%) 4.5264\n",
      "Iteration 50\n",
      "44m 42s (- 849m 32s) (50 5%) 7.9301\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoder(hidden_size, output_lang.n_words, dropout=0.1).to(device)\n",
    "\n",
    "plot_losses = trainIters(encoder, decoder, 1000, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c87222f",
   "metadata": {},
   "source": [
    "#### Save the model\n",
    "\n",
    "Save the model both as state_dict and the entire model.\n",
    "\n",
    "Not sure about this as there is no \"the model\". I am assuming that this refers to the encoder and the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c24cfd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder_weights.pth')\n",
    "torch.save(decoder.state_dict(), 'decoder_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec9dbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder, 'encoder.pth')\n",
    "torch.save(decoder, 'decoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee97bde",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "\n",
    "Logically it is similar to the train method. But there is no target. So, we feed the decoder’s output as decoder’s input of the next time step. Let’s define a method infer for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5b1ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70e2a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        output_words, attentions = infer(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        with open('evaluation_input.txt','a') as out:\n",
    "            out.write('{}, {}\\n'.format(pair[1],output_sentence))\n",
    "            \n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfc41b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27f198",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092b8e6",
   "metadata": {},
   "source": [
    "### Evaluate summarization results with ROUGE score\n",
    "\n",
    "Install the rouge-score package pip install rouge-score.\n",
    "\n",
    "Write a python program that:\n",
    "\n",
    "- Reads each line from the evaluation_input.txt and creates a list of tuple input_pair. Each tuple consists of the first and last sentences from each line of the evaluation_input.txt.\n",
    "\n",
    "- Write a function called scoring(input_pair) that instantiates the rouge_scorer object as scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True).\n",
    "\n",
    "- And for each input pair, it calls the score function as:\n",
    "\n",
    "    for pair in input_pair:\n",
    "\n",
    "        scores = scorer.score(pair[0],pair[1])\n",
    "\n",
    "- Write these scores to a file\n",
    "\n",
    "Sample output from this should look like the following:\n",
    "\n",
    "{'rouge1': Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765)}\n",
    "\n",
    "{'rouge1': Score(precision=1.0, recall=0.5, fmeasure=0.6666666666666666)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e80c60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "def read_input(filename = 'evaluation_input.txt'):\n",
    "\n",
    "    with open('evaluation_input.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    input_pair = []\n",
    "    sentences = []\n",
    "    targets = [] \n",
    "    \n",
    "    for line in lines:\n",
    "        pair = line.split(',')\n",
    "        sentences.append(pair[0])\n",
    "        targets.append(pair[1])\n",
    "    \n",
    "    input_pair.append(sentences)\n",
    "    input_pair.append(targets)\n",
    "    \n",
    "    return input_pair\n",
    "\n",
    "\n",
    "def write_score(scores):\n",
    "    # write scores to a file. This file is the out of this milestone\n",
    "\n",
    "    with open('rouge_scores.txt', 'a') as f:\n",
    "        f.write(str(scores)+'\\n')\n",
    "    f.close()\n",
    "\n",
    "def scoring(input_pair):\n",
    "\n",
    "    with open('rouge_scores.txt', 'w') as f:\n",
    "        f.truncate()       \n",
    "    f.close()\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "    sentences = []\n",
    "    targets = []\n",
    "    sentences.extend(input_pair[0])\n",
    "    targets.extend(input_pair[1])\n",
    "    for pair in zip(sentences, targets):\n",
    "        scores = scorer.score(pair[0],pair[1])\n",
    "        write_score(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6ff099c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Coronavirus: Chinese Nationals On Holidays To Remain In China Or Face Quarantine – LASG',\n",
       "  'Federal official: Coronavirus risk low',\n",
       "  'VERIFY: Are videos showing people eating bats the reason for the coronavirus outbreak',\n",
       "  'Corona Virus: Austrian Airlines Suspends Flights to Mainland China',\n",
       "  'Flight evacuating US nationals from China amid coronavirus outbreak diverted to Calif. air base | News & Features',\n",
       "  'Are you in danger of catching the coronavirus? 5 questions answered',\n",
       "  'Thailand to screen all airline passengers from China for coronavirus',\n",
       "  'Coronavirus Update: 2 More New Yorkers Tested',\n",
       "  'Gov’t sets forth plan to ease shortage of surgical masks',\n",
       "  'Once millones de personas en cuarentena en Wuhan para frenar al coronavirus'],\n",
       " [' Coronavirus: Health to as Coronavirus <EOS>\\n',\n",
       "  ' finding vaccine started',\n",
       "  ' Coronavirus: to of coronavirus to <EOS>\\n',\n",
       "  ' Coronavirus: to as Coronavirus to as China <EOS>\\n',\n",
       "  ' Coronavirus: to as China virus <EOS>\\n',\n",
       "  ' Coronavirus: Health to as China virus <EOS>\\n',\n",
       "  ' Coronavirus: Health to as Coronavirus <EOS>\\n',\n",
       "  ' Governor Says | Patch',\n",
       "  ' Coronavirus to as China to virus <EOS>\\n',\n",
       "  ' Coronavirus: to as China virus <EOS>\\n']]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3df0f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pair = [['The quick brown fox jumps over the lazy dog', 'The quick brown dog jumps on the log.'],['my name is fox', 'my name']]\n",
    "scoring(input_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "86a01037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.25, recall=0.1111111111111111, fmeasure=0.15384615384615383)}\r\n",
      "{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\r\n"
     ]
    }
   ],
   "source": [
    "!cat rouge_scores.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "01863b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring(read_input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "165cddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.3333333333333333, recall=0.15384615384615385, fmeasure=0.21052631578947367)}\r\n",
      "{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\r\n",
      "{'rouge1': Score(precision=0.16666666666666666, recall=0.07692307692307693, fmeasure=0.10526315789473684)}\r\n",
      "{'rouge1': Score(precision=0.25, recall=0.2222222222222222, fmeasure=0.23529411764705882)}\r\n",
      "{'rouge1': Score(precision=0.5, recall=0.1875, fmeasure=0.2727272727272727)}\r\n",
      "{'rouge1': Score(precision=0.14285714285714285, recall=0.09090909090909091, fmeasure=0.1111111111111111)}\r\n",
      "{'rouge1': Score(precision=0.3333333333333333, recall=0.2, fmeasure=0.25)}\r\n",
      "{'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\r\n",
      "{'rouge1': Score(precision=0.14285714285714285, recall=0.09090909090909091, fmeasure=0.1111111111111111)}\r\n",
      "{'rouge1': Score(precision=0.16666666666666666, recall=0.08333333333333333, fmeasure=0.1111111111111111)}\r\n"
     ]
    }
   ],
   "source": [
    "!cat rouge_scores.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
