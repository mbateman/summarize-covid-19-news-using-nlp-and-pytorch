{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28670142",
   "metadata": {},
   "source": [
    "# Summarizing Covid-19 News Using NLP and Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0ea67bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os, glob\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bdd265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7fac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "webhose_2019_12 = 'datasubset/16119_webhose_2019_12_db21c91a1ab47385bb13773ed8238c31_0000001.json'\n",
    "webhose_2020_01 = 'datasubset/16119_webhose_2020_01_db21c91a1ab47385bb13773ed8238c31_0000001.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a7621",
   "metadata": {},
   "source": [
    "## Download and extract the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac48fd",
   "metadata": {},
   "source": [
    "Read each of those files, extract the value of the text key and title key from those objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a27771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "target = []\n",
    "for filename in [webhose_2019_12, webhose_2020_01]:\n",
    "    with open(filename, 'r') as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "    for json_str in json_list:\n",
    "        result = json.loads(json_str)\n",
    "        dataset.append(result['text'])\n",
    "        target.append(result['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739234d6",
   "metadata": {},
   "source": [
    "The length of the list dataset and target will be 94403. So essentially our dataset size is about 100K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f516f421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94403, 94403)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset), len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b229d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Global Swine Healthcare Market by Products, Diseases & Geography – Forecast to 2024',\n",
       " 'FDA launches app for health care professionals to report novel uses of existing medicines for patients with difficult-to-treat infectious diseases',\n",
       " 'C-Suite Awards: Regina Yan',\n",
       " 'FDA Launches Infectious Disease Crowdsourcing App for Clinicians FDA Launches Infectious Disease Crowdsourcing App for Clinicians',\n",
       " 'Drug Safety Oversight Board',\n",
       " 'How Prepared Are We For The Next Pandemic? Not Very, Experts Show',\n",
       " 'Suspected MERS case reported',\n",
       " 'Factors associated with and barriers to disclosure of a sexual assault to formal on-campus resources among college students - Mennicke A, Bowling J, Gromer J, Ryan C.',\n",
       " \"The effect of university students' violence tendency on their attitude towards domestic violence and the factors affecting domestic violence attitudes - Yagiz R, Sevil U, Guner Ö.\",\n",
       " 'YoYo Discusses Career, Teaching and Female Empowerment']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e3387",
   "metadata": {},
   "source": [
    "## Text cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5f3f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contraction_hashmap import contraction_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03a3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.split() # convert have'nt -> have not\n",
    "    for i in range(len(text)):\n",
    "        word = text[i]\n",
    "        if word in contraction_map:\n",
    "            text[i] = contraction_map[word]\n",
    "    text = \" \".join(text)\n",
    "    text = text.split()\n",
    "    newtext = []\n",
    "    for word in text:\n",
    "        if word not in stop_words:\n",
    "            newtext.append(word)\n",
    "    text = \" \".join(newtext)\n",
    "    text = text.replace(\"'s\",'') # convert your's -> your\n",
    "    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n",
    "    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n",
    "    text = re.sub(r'\\.',' . ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ad25071",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [preprocess(text) for text in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afde2f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94403"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5040170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [preprocess(text) for text in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e90ddda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94403"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b1947d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text = 600\n",
    "max_len_target = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05c99e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    if(len(target[i].split())<=max_len_target and len(dataset[i].split())<=max_len_text):\n",
    "        short_text.append(dataset[i])\n",
    "        short_summary.append(target[i])\n",
    "\n",
    "temp_df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee7bb716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA launches app for health care professionals...</td>\n",
       "      <td>FDA launches app for health care professionals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Of all of Regina Yan ’s many traits, an open m...</td>\n",
       "      <td>C-Suite Awards: Regina Yan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The CURE ID app allows clinicians to share and...</td>\n",
       "      <td>FDA Launches Infectious Disease Crowdsourcing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The DSB is composed of representatives from tw...</td>\n",
       "      <td>Drug Safety Oversight Board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Centre for Health Protection (CHP) of the ...</td>\n",
       "      <td>Suspected MERS case reported</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  FDA launches app for health care professionals...   \n",
       "1  Of all of Regina Yan ’s many traits, an open m...   \n",
       "2  The CURE ID app allows clinicians to share and...   \n",
       "3  The DSB is composed of representatives from tw...   \n",
       "4  The Centre for Health Protection (CHP) of the ...   \n",
       "\n",
       "                                             summary  \n",
       "0  FDA launches app for health care professionals...  \n",
       "1                         C-Suite Awards: Regina Yan  \n",
       "2  FDA Launches Infectious Disease Crowdsourcing ...  \n",
       "3                        Drug Safety Oversight Board  \n",
       "4                       Suspected MERS case reported  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd40fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64893 entries, 0 to 64892\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     64893 non-null  object\n",
      " 1   summary  64893 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1014.1+ KB\n"
     ]
    }
   ],
   "source": [
    "temp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52b8743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = temp_df[temp_df['summary'].str.strip().astype(bool)]\n",
    "df = newdf[newdf['text'].str.strip().astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dafbb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62358 entries, 0 to 64892\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   text     62358 non-null  object\n",
      " 1   summary  62358 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97717a6f",
   "metadata": {},
   "source": [
    "## Text feature generation\n",
    "\n",
    "Now that we have done the text cleanup, we need to convert the text into numerical representations to be used by the model. This process is called feature generation. There are different ways to generate features out of text data. Here we will use one-hot vector[3] technique with some tweaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774afebe",
   "metadata": {},
   "source": [
    "### Define a class Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "182009c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252cce7f",
   "metadata": {},
   "source": [
    "## Make the features ready for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2044c1a",
   "metadata": {},
   "source": [
    "### Define a function readData(text, summary) \n",
    "\n",
    "This takes text and summary as input. Here text and summary are two lists of strings. When we call this readData function, we will call it with our cleaned data X and Y respectively. This function does the following operations:\n",
    "Creates a tuple from text and summary as in pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "Creates input and output object by passing text and summary to the Lang class Note that we are only creating objects here. Not executing any other functions from the Lang class.\n",
    "Return input, output, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f23b312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(text, summary):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "    \n",
    "    input_lang = Lang(text)\n",
    "    output_lang = Lang(summary)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549943e",
   "metadata": {},
   "source": [
    "### Define a function prepareData that takes list(df['text']) and list(df['summary']) as input.\n",
    "\n",
    "This prepareData function calls readData(X,Y) and gets back input, output, and pairs\n",
    "For each item in the pairs list, we will do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c2e0caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2):\n",
    "    input_lang, output_lang, pairs = readData(lang1, lang2)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04995f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 62358 sentence pairs\n",
      "Counting words...\n",
      "['REGINA - A Saskatchewan university has cancelled all China trips it has organized for the next three months due to the spread of the coronavirus.\\nUniversity of Regina spokesman Paul Dederick says anyone who would have been involved in the travel has been advised.\\nHe says, effective immediately, all other travel to China by students, staff, faculty as part of student exchanges or research partnerships will require the dean\\'s approval and must include a plan on how to decrease health risks.\\nDederick says the measures are precautionary as the federal government has issued a travel advisory for China.\\nHealth officials in Ontario have confirmed Canada\\'s first case of the coronavirus and believe the patient\\'s wife to be the second case.\\nThey say the risk to Canadians is low.\\n\"As a precautionary measure the university is taking a pro-active approach to ensure faculty, staff and students travelling to and from China are aware of, and take steps to mitigate, their risk of contracting or spreading the virus,\" Dederick said in an email.\\nChina is reporting that the illness has infected more than 2,700 people and killed at least 81.\\nThis report by The Canadian Press was first published Jan. 27, 2020 Advertisement', 'University of Regina cancels travel to China due to coronavirus']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(list(df['text']), list(df['summary']))\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dce977",
   "metadata": {},
   "source": [
    "## Deliverable\n",
    "\n",
    "The deliverable is a Jupyter Notebook documenting your workflow. The end result of this notebook is a list of pairs of sentences. The 1st column in each row of this list is the text sentence and the 2nd column is the target/summary sentence. A sample output below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95a9d6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDA launches app for health care professionals to report novel uses of existing medicines for patients with difficult-to-treat infectious diseases FDA launches app for health care professionals to report novel uses of existing medicines for patients with difficult-to-treat infectious diseases Authors: FDA Pinworms of the red howler monkey (Alouatta seniculus) in Colombia: gathering the pieces of the pinworm-primate puzzle Publication date: Available online 4 December 2019Source: International Journal for Parasitology: Parasites and WildlifeAuthor(s): Brenda Solórzano-García, Andrés Link Ospina, Silvia Rondón, Gerardo Pérez-Ponce de LeónAbstractPinworms of primates are believed to be highly host specific parasites, forming co-evolutionary associations with their hosts. In order to assess the strength and reach of such evolutionary links, we need to have a broad understanding of the pinworm diversity associated with primates. Here, we employed an integrative taxonomic approach to assess pinworm divers... Parasitology Influences of cyclosporin A and non-immunosuppressive derivatives on cellular cyclophilins and viral nucleocapsid protein during human coronavirus 229E replication Publication date: January 2020Source: Antiviral Research, Volume 173Author(s): Yue Ma-Lauer, Yu Zheng, Miroslav Malešević, Brigitte von Brunn, Gunter Fischer, Albrecht von BrunnAbstractThe well-known immunosuppressive drug cyclosporin A inhibits replication of various viruses including coronaviruses by binding to cellular cyclophilins thus inactivating their cis-trans peptidyl-prolyl isomerase function. Viral nucleocapsid proteins are inevitable for genome encapsidation and replication. Here we demonstrate the interaction between the N protein of HCoV-229E and cyclophilin A, not cyclophilin B. Cyclophilin inhibitor...\n",
      "\n",
      "FDA launches app for health care professionals to report novel uses of existing medicines for patients with difficult-to-treat infectious diseases\n"
     ]
    }
   ],
   "source": [
    "print(pairs[0][0])\n",
    "print()\n",
    "print(pairs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b18669",
   "metadata": {},
   "source": [
    "## Build an Attention Based Deep Learning Model for Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c9fca",
   "metadata": {},
   "source": [
    "### Define a Sequence-to-Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55cbc6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = max_len_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4298cd05",
   "metadata": {},
   "source": [
    "#### Define the encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ceceb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0b554",
   "metadata": {},
   "source": [
    " #### Define the class AttnDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "387ccf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88ff523",
   "metadata": {},
   "source": [
    "#### Convert the training data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d295965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b6184",
   "metadata": {},
   "source": [
    "### Train a Sequence-to-Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d8de4",
   "metadata": {},
   "source": [
    "#### The train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf3fbbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(input_length):\n",
    "        try:\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
    "            encoder_outputs[i] = encoder_output[0, 0]\n",
    "        except IndexError:\n",
    "            print('Index Error in train')\n",
    "            print('index=',i)\n",
    "            print('input_length', input_length)\n",
    "            print('target_length', target_length)\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982b26a",
   "metadata": {},
   "source": [
    "#### The trainIters method\n",
    "\n",
    "The trainIters method has 4 important parameters. You can have more parameters for debugging/logging purposes.\n",
    "\n",
    "- encoder: The encoder object of the Encoder class\n",
    "- decoder: The decoder object of the AttnDecoder class\n",
    "- num_iters: Number of iterations you want to train using the train method. This is an integer number.\n",
    "- learning_rate: Learning rate hyperparameter for your neural network. Feel free to use a default value for this parameter.\n",
    "\n",
    "We need to define the optimizers for encoder and decoder objects. These optimizers are gradient descent optimizers.\n",
    "\n",
    "Convert the training pairs to tensors\n",
    "\n",
    "Define the loss function. We are solving a classification problem, so we have to use a loss function that is commonly used in classification problems: log loss. In particular, here we will use negative log loss as criterion = nn.NLLLoss(). Refer to Understanding how LSTM works.\n",
    "\n",
    "Finally, we will call the train method num_iters times.\n",
    "\n",
    "We can save this loss output to look at the training loss over time. This helps us to debug the model and makes sure our model is improving over time.\n",
    "\n",
    "Add some log messages in the beginning and end of this method to keep track of the start and end of training such as starting training ... and end training ... as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d596be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97f3c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    print('Starting training ...')\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('Iteration', iter)\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    print('Stopping training ...')\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87385677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db51c4",
   "metadata": {},
   "source": [
    "#### Now actually train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "245c86d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training ...\n",
      "Iteration 50\n",
      "6m 11s (- 55m 41s) (50 10%) 7.0218\n",
      "Iteration 100\n",
      "12m 36s (- 50m 25s) (100 20%) 5.1913\n",
      "Iteration 150\n",
      "18m 1s (- 42m 3s) (150 30%) 6.5851\n",
      "Iteration 200\n",
      "24m 17s (- 36m 26s) (200 40%) 6.1937\n",
      "Iteration 250\n",
      "30m 53s (- 30m 53s) (250 50%) 6.3365\n",
      "Iteration 300\n",
      "36m 24s (- 24m 16s) (300 60%) 6.3908\n",
      "Iteration 350\n",
      "42m 58s (- 18m 25s) (350 70%) 6.4690\n",
      "Iteration 400\n",
      "48m 59s (- 12m 14s) (400 80%) 6.9316\n",
      "Iteration 450\n",
      "55m 0s (- 6m 6s) (450 90%) 6.8060\n",
      "Iteration 500\n",
      "61m 51s (- 0m 0s) (500 100%) 6.4888\n",
      "Stopping training ...\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoder(hidden_size, output_lang.n_words, dropout=0.1).to(device)\n",
    "\n",
    "plot_losses = trainIters(encoder, decoder, 500, print_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c87222f",
   "metadata": {},
   "source": [
    "#### Save the model\n",
    "\n",
    "Save the model both as state_dict and the entire model.\n",
    "\n",
    "Not sure about this as there is no \"the model\". I am assuming that this refers to the encoder and the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57357ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'encoder_weights.pth')\n",
    "torch.save(decoder.state_dict(), 'decoder_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec9dbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder, 'encoder.pth')\n",
    "torch.save(encoder, 'decoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee97bde",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "\n",
    "Logically it is similar to the train method. But there is no target. So, we feed the decoder’s output as decoder’s input of the next time step. Let’s define a method infer for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5b1ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70e2a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inferRandomly(encoder, decoder, n=10):\n",
    "    f = open(\"evaluation_input.txt\", \"w\")\n",
    "    \n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        output_words, attentions = infer(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        f.write(pair[1]+','+output_sentence+'\\n')\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfc41b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferRandomly(encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
